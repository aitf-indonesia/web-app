from ddgs import DDGS
import httpx
from bs4 import BeautifulSoup
import json
import os
import sys
import argparse
from datetime import datetime
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from multiprocessing import Pool, cpu_count
from sqlalchemy import create_engine, text
from dotenv import load_dotenv


# Load environment variables
load_dotenv()

# Configuration
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "output")
OUTPUT_IMG_DIR = os.path.join(OUTPUT_DIR, "img")
LAST_ID_FILE = os.path.join(OUTPUT_DIR, "last_id.txt")
ALL_DOMAINS_FILE = os.path.join(OUTPUT_DIR, "all_domains.txt")
BLOCKED_DOMAINS_FILE = os.path.join(OUTPUT_DIR, "..", "blocked_domains.txt")  # Relative to crawler root

# Database configuration
DATABASE_URL = os.getenv("DB_URL", "postgresql://postgres:root@localhost:5432/prd")
engine = create_engine(DATABASE_URL)

# Timeout configuration (in seconds)
FETCH_TIMEOUT = 10
OG_TIMEOUT = 10
SCREENSHOT_TIMEOUT = 20

# Processing configuration
MAX_WORKERS_FETCH = 5
MAX_WORKERS_SCREENSHOT = max(2, cpu_count() - 1)  # Use multiple CPU cores for screenshots
MAX_RESULT = 10  # Maximum number of valid domains to process per run
VERSION = "1.4"

# Global sets untuk tracking (anti-duplikasi dan blocked domains)
SEEN_DOMAINS = set()
BLOCKED_DOMAINS = set()

# Ensure output directories exist
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(OUTPUT_IMG_DIR, exist_ok=True)


def get_last_id():
    """Get last ID from file or return -1 if not exists."""
    if os.path.exists(LAST_ID_FILE):
        try:
            with open(LAST_ID_FILE, "r") as f:
                return int(f.read().strip())
        except:
            return -1
    return -1


def load_seen_domains():
    """Load all previously seen domains into global set."""
    global SEEN_DOMAINS
    SEEN_DOMAINS = set()
    
    if os.path.exists(ALL_DOMAINS_FILE):
        try:
            with open(ALL_DOMAINS_FILE, "r", encoding="utf-8") as f:
                for line in f:
                    domain = line.strip()
                    if domain:
                        SEEN_DOMAINS.add(domain)
            print(f"[INFO] Loaded {len(SEEN_DOMAINS)} existing domains from {ALL_DOMAINS_FILE}")
        except Exception as e:
            print(f"[WARNING] Failed to load domains: {str(e)}")
    else:
        print(f"[INFO] No existing domains file. Starting fresh.")


def load_blocked_domains():
    """Load blocked domains from file into global set."""
    global BLOCKED_DOMAINS
    BLOCKED_DOMAINS = set()
    
    # Convert relative path to absolute
    blocked_file = os.path.abspath(BLOCKED_DOMAINS_FILE)
    
    if os.path.exists(blocked_file):
        try:
            with open(blocked_file, "r", encoding="utf-8") as f:
                for line in f:
                    domain = line.strip()
                    if domain:
                        BLOCKED_DOMAINS.add(domain)
            print(f"[INFO] Loaded {len(BLOCKED_DOMAINS)} blocked domains from {blocked_file}")
        except Exception as e:
            print(f"[WARNING] Failed to load blocked domains: {str(e)}")
    else:
        print(f"[WARNING] Blocked domains file not found at {blocked_file}. Continuing without blocked list.")


def save_last_id(last_id):
    """Save the last ID to file."""
    with open(LAST_ID_FILE, "w") as f:
        f.write(str(last_id))


def extract_domain(url):
    """Extract domain from URL."""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc
        return domain if domain else "unknown"
    except:
        return "unknown"


def is_domain_blocked(domain):
    """Check if domain is in blocked list."""
    if not domain or domain == "unknown":
        return False
    
    # Check if domain or any subdomain matches blocked domains
    for blocked in BLOCKED_DOMAINS:
        if blocked in domain:
            return True
    return False


def is_domain_duplicate(domain):
    """Check if domain already exists in global set."""
    return domain in SEEN_DOMAINS


def add_domain_to_set(domain):
    """Add domain to global set."""
    if domain and domain != "unknown":
        SEEN_DOMAINS.add(domain)


def save_new_domains(new_domains):
    """Append new domains to all_domains.txt file."""
    if not new_domains:
        return
    
    try:
        with open(ALL_DOMAINS_FILE, "a", encoding="utf-8") as f:
            for domain in new_domains:
                f.write(domain + "\n")
        print(f"[INFO] Saved {len(new_domains)} new domains to {ALL_DOMAINS_FILE}")
    except Exception as e:
        print(f"[ERROR] Failed to save new domains: {str(e)}")


def get_og_data(html):
    """Extract OG meta tags from HTML."""
    try:
        soup = BeautifulSoup(html, "html.parser")

        def og(prop):
            tag = soup.find("meta", property=f"og:{prop}")
            return tag["content"] if tag and tag.get("content") else None

        return {
            "og:title": og("title"),
            "og:description": og("description"),
            "og:type": og("type"),
            "og:site_name": og("site_name"),
        }
    except Exception as e:
        return {
            "og:title": f"Error: {str(e)}",
            "og:description": f"Error: {str(e)}",
            "og:type": f"Error: {str(e)}",
            "og:site_name": f"Error: {str(e)}",
        }


def take_screenshot_worker(url, output_path, item_id):
    """Worker function for taking screenshot (must be picklable for multiprocessing)."""
    try:
        options = Options()
        options.binary_location = "/usr/bin/chromium-browser"
        options.add_argument("--headless=new")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-software-rasterizer")
        options.add_argument("--disable-extensions")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36")
        
        service = Service("/usr/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=options)
        
        # Set timeouts from configuration
        driver.set_page_load_timeout(SCREENSHOT_TIMEOUT)
        driver.set_script_timeout(SCREENSHOT_TIMEOUT)
        
        driver.get(url)
        time.sleep(2)
        driver.save_screenshot(output_path)
        driver.quit()
        
        return True
        
    except Exception as e:
        # No retry on timeout - just return False and continue
        return False


def take_screenshot(url, output_path, retries=2):
    """Take screenshot of the URL (no retries on timeout)."""
    try:
        options = Options()
        options.binary_location = "/usr/bin/chromium-browser"
        options.add_argument("--headless=new")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-software-rasterizer")
        options.add_argument("--disable-extensions")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36")
        
        service = Service("/usr/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=options)
        
        # Set timeouts from configuration
        driver.set_page_load_timeout(SCREENSHOT_TIMEOUT)
        driver.set_script_timeout(SCREENSHOT_TIMEOUT)
        
        driver.get(url)
        time.sleep(2)
        driver.save_screenshot(output_path)
        driver.quit()
        
        return True
        
    except Exception as e:
        # No retry on timeout - just return False
        return False


def fetch_url_data(url_item, item_index, current_id):
    """Fetch data for a single URL (without screenshot)."""
    result = {
        "id": f"{current_id:08d}",
        "title": url_item.get("title", "-"),
        "url": url_item.get("href", "-"),
        "domain": extract_domain(url_item.get("href", "-")),
        "description": url_item.get("body", "-"),
        "og_metadata": {},
        "screenshot_status": "pending",
    }

    url = url_item.get("href", "")

    if url:
        try:
            resp = httpx.get(url, timeout=FETCH_TIMEOUT, follow_redirects=True)
            resp.raise_for_status()
            html = resp.text
            result["og_metadata"] = get_og_data(html)

        except Exception as e:
            # Include timeout in error message
            result["og_metadata"] = {
                "og:title": f"Error: {str(e)[:100]}",
                "og:description": f"Error: {str(e)[:100]}",
                "og:type": f"Error: {str(e)[:100]}",
                "og:site_name": f"Error: {str(e)[:100]}",
            }

    print(f"[{item_index}] Fetched ID {current_id:08d}: {result['title'][:50]}")
    return result


def process_screenshots_parallel(all_results):
    """Process screenshots in parallel using multiprocessing."""
    print(f"\n=== TAKING SCREENSHOTS (Parallel - {MAX_WORKERS_SCREENSHOT} processes) ===")
    
    # Prepare screenshot tasks
    screenshot_tasks = []
    for result in all_results:
        url = result.get("url", "")
        item_id = result.get("id", "unknown")
        
        if url and url != "-":
            screenshot_path = os.path.join(OUTPUT_IMG_DIR, f"{item_id}.png")
            screenshot_tasks.append((url, screenshot_path, item_id))
    
    # Use ProcessPoolExecutor for parallel screenshot processing
    results_status = {}
    
    with ProcessPoolExecutor(max_workers=MAX_WORKERS_SCREENSHOT) as executor:
        futures = {
            executor.submit(take_screenshot_worker, url, path, item_id): item_id
            for url, path, item_id in screenshot_tasks
        }
        
        completed = 0
        for future in as_completed(futures):
            item_id = futures[future]
            try:
                success = future.result()
                results_status[item_id] = "success" if success else "failed"
                completed += 1
                status_symbol = "✓" if success else "✗"
                print(f"[{completed}/{len(screenshot_tasks)}] {item_id} {status_symbol}")
            except Exception as e:
                results_status[item_id] = "failed"
                completed += 1
                print(f"[{completed}/{len(screenshot_tasks)}] {item_id} ✗ (Exception: {str(e)[:50]})")
    
    # Update screenshot status in results
    for result in all_results:
        item_id = result.get("id", "unknown")
        if item_id in results_status:
            result["screenshot_status"] = results_status[item_id]
        else:
            result["screenshot_status"] = "skipped"


def save_to_database(all_results, keyword):
    """Save crawled results to database."""
    print("[DATABASE] Starting database save...", flush=True)
    
    try:
        with engine.begin() as conn:
            saved_count = 0
            for result in all_results:
                # Prepare image path in the format: keyword-based-crawler/output/img/<id>.png
                image_path = f"keyword-based-crawler/output/img/{result['id']}.png" if result.get('screenshot_status') == 'success' else None
                
                # Convert og_metadata dict to JSON string for JSONB column
                og_metadata_json = json.dumps(result.get('og_metadata', {}))
                
                # Insert into crawling_data table
                # Use CAST for JSONB to be compatible with SQLAlchemy parameter binding
                conn.execute(text("""
                    INSERT INTO crawling_data (url, title, description, domain, og_metadata, image_path, status)
                    VALUES (:url, :title, :description, :domain, CAST(:og_metadata AS jsonb), :image_path, :status)
                """), {
                    "url": result.get('url', ''),
                    "title": result.get('title', '')[:255],  # Limit to 255 chars
                    "description": result.get('description', ''),
                    "domain": result.get('domain', ''),
                    "og_metadata": og_metadata_json,
                    "image_path": image_path,
                    "status": "pending"
                })
                saved_count += 1
            
            print(f"[DATABASE] Successfully saved {saved_count} records to database", flush=True)
            return True
            
    except Exception as e:
        print(f"[DATABASE] ERROR: Failed to save to database - {str(e)}", flush=True)
        import traceback
        print(f"[DATABASE] Traceback: {traceback.format_exc()}", flush=True)
        return False




def main():
    # Parse command-line arguments
    parser = argparse.ArgumentParser(description='Keyword-based web crawler')
    parser.add_argument('-n', '--domain-count', type=int, default=10, help='Number of domains to generate')
    parser.add_argument('-k', '--keywords', type=str, help='Comma-separated list of keywords')
    args = parser.parse_args()
    
    # Track start time
    start_time = time.time()
    
    # Load existing domains and blocked domains first
    print("[INIT] Loading existing domains and blocked domains...", flush=True)
    load_seen_domains()
    load_blocked_domains()
    print(f"[INIT] Loaded {len(SEEN_DOMAINS)} existing domains, {len(BLOCKED_DOMAINS)} blocked domains", flush=True)
    
    # Get keywords - either from args or stdin
    if args.keywords:
        keywords = [k.strip() for k in args.keywords.split(',') if k.strip()]
    else:
        keyword_input = input("Masukkan keyword (pisahkan dengan koma): ")
        keywords = [k.strip() for k in keyword_input.split(',') if k.strip()]
    
    if not keywords:
        print("[ERROR] No keywords provided", flush=True)
        return
    
    # Use domain count from args
    target_domains = args.domain_count
    global MAX_RESULT
    MAX_RESULT = target_domains
    
    print(f"[CONFIG] Keywords: {', '.join(keywords)}", flush=True)
    print(f"[CONFIG] Target domains: {target_domains}", flush=True)
    
    # Combine all keywords for search
    query = ' OR '.join(keywords)
    print(f"[SEARCH] Starting search with query: {query}", flush=True)
    
    # Get search results
    print("[SEARCH] Fetching search results from DuckDuckGo...", flush=True)
    results = DDGS().text(query, max_results=target_domains * 5)  # Get more results to handle filtering
    
    if not results:
        print("[ERROR] No search results found", flush=True)
        return
    
    print(f"[SEARCH] Found {len(results)} search results", flush=True)
    
    # Get last ID and start from next
    last_id = get_last_id()
    current_id = last_id + 1
    
    print(f"[INIT] Starting ID: {current_id:08d}", flush=True)
    print(f"[FILTER] Filtering domains (target: {target_domains})...", flush=True)
    
    # Process results until we have target_domains valid domains
    new_domains_list = []
    filtered_no_duplicates = []
    
    for r in results:
        if len(filtered_no_duplicates) >= target_domains:
            break
        
        domain = extract_domain(r.get("href", "-"))
        
        # Check various conditions to skip URL
        if domain == "unknown":
            print(f"[FILTER] Skipped invalid domain from URL: {r.get('href', '-')[:50]}...", flush=True)
            continue
        
        if is_domain_blocked(domain):
            print(f"[FILTER] Skipped blocked domain: {domain}", flush=True)
            continue
        
        if is_domain_duplicate(domain):
            print(f"[FILTER] Skipped duplicate domain: {domain}", flush=True)
            continue
        
        # All checks passed - this is a valid domain
        filtered_no_duplicates.append(r)
        new_domains_list.append(domain)
        add_domain_to_set(domain)
        print(f"[FILTER] Added domain ({len(filtered_no_duplicates)}/{target_domains}): {domain}", flush=True)
    
    if not filtered_no_duplicates:
        print("[ERROR] No valid domains found after filtering", flush=True)
        return
    
    print(f"[FILTER] Filtering complete. Total valid domains: {len(filtered_no_duplicates)}", flush=True)
    
    # Process URLs with multithreading for fetching
    print(f"[FETCH] Starting to fetch {len(filtered_no_duplicates)} URLs...", flush=True)
    all_results = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS_FETCH) as executor:
        futures = {
            executor.submit(fetch_url_data, url_item, idx + 1, current_id + idx): idx
            for idx, url_item in enumerate(filtered_no_duplicates)
        }
        
        for future in as_completed(futures):
            result = future.result()
            all_results.append(result)
            print(f"[FETCH] Fetched {len(all_results)}/{len(filtered_no_duplicates)}: {result.get('domain', 'unknown')}", flush=True)
    
    # Sort by ID to maintain order
    all_results.sort(key=lambda x: int(x["id"]))
    print(f"[FETCH] Fetch complete. Total fetched: {len(all_results)}", flush=True)
    
    # Count OG metadata success
    og_success = sum(1 for r in all_results if r.get('og_metadata', {}).get('og:title') and 'Error' not in str(r.get('og_metadata', {}).get('og:title', '')))
    print(f"[FETCH] OG Metadata successfully fetched: {og_success}/{len(all_results)}", flush=True)
    
    # Process screenshots in parallel (multiprocessing)
    print(f"[SCREENSHOT] Starting screenshot capture for {len(all_results)} URLs...", flush=True)
    process_screenshots_parallel(all_results)
    
    # Generate timestamp
    now = datetime.utcnow()
    timestamp_iso = now.isoformat() + "Z"
    timestamp_filename = now.strftime("%d%m%y-%H%M")
    
    # Prepare metadata
    metadata = {
        "metadata": {
            "total_records": len(all_results),
            "generated_at": timestamp_iso,
            "version": VERSION,
            "keywords": keywords,
        },
        "data": all_results,
    }
    
    # Save to JSON
    json_filename = f"{timestamp_filename}.json"
    json_filepath = os.path.join(OUTPUT_DIR, json_filename)
    
    with open(json_filepath, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"[SAVE] JSON saved: {json_filepath}", flush=True)
    
    # Update last ID
    final_id = current_id + len(all_results) - 1
    save_last_id(final_id)
    print(f"[SAVE] Last ID updated: {final_id:08d}", flush=True)
    
    # Save new domains to file
    save_new_domains(new_domains_list)
    print(f"[SAVE] Saved {len(new_domains_list)} new domains to tracking file", flush=True)
    
    # Save to database
    print(f"[DATABASE] Saving {len(all_results)} records to database...", flush=True)
    db_success = save_to_database(all_results, ', '.join(keywords))
    
    # Count screenshot results
    screenshot_success = sum(1 for r in all_results if r.get("screenshot_status") == "success")
    screenshot_failed = sum(1 for r in all_results if r.get("screenshot_status") == "failed")
    screenshot_skipped = sum(1 for r in all_results if r.get("screenshot_status") == "skipped")
    
    # Calculate elapsed time
    elapsed_time = int(time.time() - start_time)
    elapsed_minutes = elapsed_time // 60
    elapsed_seconds = elapsed_time % 60
    
    # Print summary as JSON for easy parsing
    summary = {
        "status": "success" if db_success else "partial_success",
        "timestamp": timestamp_iso,
        "time_elapsed": f"{elapsed_minutes:02d}:{elapsed_seconds:02d}",
        "time_elapsed_seconds": elapsed_time,
        "fetch_domain": {
            "success": len(all_results),
            "total": target_domains
        },
        "fetch_og_metadata": {
            "success": og_success,
            "total": len(all_results)
        },
        "screenshot": {
            "success": screenshot_success,
            "failed": screenshot_failed,
            "skipped": screenshot_skipped,
            "total": len(all_results)
        },
        "domains_inserted": len(all_results) if db_success else 0,
        "keywords": keywords
    }
    
    print("[SUMMARY] " + json.dumps(summary), flush=True)
    print("[DONE]", flush=True)


if __name__ == "__main__":
    main()
